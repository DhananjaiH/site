### Week 2: AI & Racial Bias
_09/01/2019_

Propublica's series on Machine and Algorithmic bias touches on the current-day implementation of "Artificial Intelligence" (read as Machine Learning by those from a Computer Science or Mathematics background) and how they affect human society. We see examples of this everyday - some are quite prominent, while others are inconspicuous altogether and secretly work against us. Just as mentioned in the series, I have noticed the discrepancies of big data algorithms at work while attempting to book airline tickets and hotels trips for business travel. Sitting at adjacent desks, my boss and I consistently observe varying prices for the same destinations and dates. This is presumably a result of my boss being tagged as someone willing to pay a higher price point with prior history of doing so. Since I don't travel as much, I get fed numbers that might be closer a mysterious 'baseline' cost. Another example would be varying search results from various providers, depending on the region they are searched from, as well as the subsequent encounter of advertisements related to the topic. In some cases, I have also been witness to getting ad responses to verbal conversations that did not have any computers involved at all.

While these virtual spiders seem omnipresent, they are still a nascent technology, and there are still some ways to avoid their consequences. On the technical side, I have personally opted for using technology services from more moral organizations such as DuckDuckGo, whose search engine promises privacy and no tracking; in addition to disabling and avoiding services fundamentally require tracking. But for a more large scale solution, I do a believe the technology+software+services industry requires a supervisory / regulatory body that sets higher demands of those that build them, before they are released to the public. Much like the role of the FDA is to the field of medical devices and pharmaceuticals, this body would need to review the design processes and motive behind these products, as well as push for substantive studies to be completed before it can be used on a large scale. This very point was made by the ProPublica article on criminal justice system's use of algorithms to decide who gets incarcerated. Unlike the FDA however, this regulatory body would need to be a civilian arm, one that is run truly 'For the People, and By the People', and play a key role in the governance of the cyber world. It has quickly been proven that the original government (across the world) is struggling to keep up with digital affairs in addition to their existing responsibilities. Having one less responsibility should be a welcome reprieve. Another argument for having a civilian-run regulatory body would be to keep the power of data manipulation outside of the reach of individuals (or groups of), which has also been an ongoing contention in the political landscape.

The term 'bias' is interesting in this case, since it is quite unintentional. Nora Khan's article puts this across well - this 'AI' doesn't hope to befriend us and aid us, nor does it act to intentionally harm us. As a virtual being, it has no real conscious, and is thus indifferent to humans, with a singular focus on performing its function and achieving its goal. Through this article, and by extension, writer Nick Bostrom suggest that ASI is a sort of force of nature whose magnitude and form are truly far beyond human cognitive abilities, including those that conceived it in the first place. Which in turn makes Ayanna Howard's study of human trust in robots and AIs seem a little contrarian - if this ASI (in its mature state) has nothing of the sort of human characteristics as we come to expect, then trust as an anthropomorphic characteristic would never be achievable. Unless - this Trust was a function that it is required to perform, in order to achieve its goals. Highlighting some of Dr. Howards research findings, this 'trust' may end up being a double-edged sword. When coupled with a multitude of business objectives, this Trust could be used in malignant ways, much like it is being used right now, with the added portent of gaining an intimate level of trust.

\-Jai

[Back to Design For This Century Home Page](http://dhananjaih.github.io/site/pages/d4tc/)

##### References:
[1] Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner. [Machine Bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing): There’s software used across the country to predict future criminals. And it’s biased against blacks; ProPublica. May 23, 2016.

[2]  [Trust in Human-Robot/AI Interactions with Ayanna Howard](https://twimlai.com/twiml-talk-110-trust-human-robot-ai-interactions-ayanna-howard/_). This Week in Machine Learning and AI (TWIML&AI). Feb 12, 2018.

[3] Julia Angwin, Terry Parris Jr. and Surya Mattu. [Machine Bias: Investigating Algorithmic Injustice](https://www.propublica.org/article/breaking-the-black-box-how-machines-learn-to-be-racist?word=Trump). ProPublica, September 28, 2016.

[4]  Khan, Nora N. [Towards a Poetics of Artificial Superintelligence](https://medium.com/after-us/towards-a-poetics-of-artificial-superintelligence-ebff11d2d249): Symbolic language can help us grasp the nature and power of what is coming. Medium. Sept 25, 2015.
